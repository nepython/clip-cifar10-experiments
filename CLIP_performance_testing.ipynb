{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# CLIP performance testing\n",
        "\n",
        "This notebook is based on the [Interacting with CLIP notebook](https://github.com/openai/CLIP/tree/main/notebooks) shared by OpenAI on CLIP's github repository. We have used the same testing setting as described in the [paper](https://arxiv.org/abs/2103.00020) for zero shot and linear probe classification. On CIFAR-10 we observe accuracy of `89.59%` against `89.83%` reported by [OpenClip](https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv#L84), a third party library. For linear probe, we observe `95.02%` which is close to `95.1%` reported in the [paper](https://arxiv.org/abs/2103.00020).\n",
        "\n",
        "We further experiment the variation in performance due to class names and sorrounding preposition text which is passed to the model. We also attempt at improving the performance by taking a mean of cosine similarity across two versions of the image (original and augmented) but couldn't notice improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "# Preparation for Colab\n",
        "\n",
        "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "06d69b82-984f-4a87-cf21-2ea8feb06cea"
      },
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-y6ngyv5y\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-y6ngyv5y\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=1598c31e7e2ff932b1911f98c781ec2561aef3b9a10e56be25ce1ef189a9a8f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ykrdw4ve/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1hkDT38hSaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91e54b6-31f3-4647-81e3-22a2075f56e9"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.6.0+cu124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-cc4b7f78b657>:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import packaging\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLFS29hnhlY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad929b6-c9c6-4575-a3cb-7c0b983a1430"
      },
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRVTY9lbGm8",
        "outputId": "5e1f4e0a-3d60-460a-b0ec-a2664a3ae516"
      },
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 200MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 151,277,313\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "## Image\n",
        "The input images are resized and center-cropped to conform with the image resolution that the model expects. Furthermore, the pixel intensity is normalised using the dataset mean and standard deviation.\n",
        "\n",
        "The second return value from `clip.load()` contains a torchvision `Transform` as shown below that performs this preprocessing.\n",
        "\n",
        "## Text\n",
        "CLIP model uses a case-insensitive text tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6cpiIFHp9N6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b248963f-aba1-4f27-ffac-972ee9d1e8a3"
      },
      "source": [
        "preprocess"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x7b07430cdbc0>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGom156-i2kL",
        "outputId": "a71b577c-cce0-4481-990f-83ad69cfe8c6"
      },
      "source": [
        "clip.tokenize(\"Hello World!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49406,  3306,  1002,   256, 49407,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantitative performance"
      ],
      "metadata": {
        "id": "_Ls0Fs3w4P64"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alePijoXy6AH"
      },
      "source": [
        "## Zero-Shot image classification\n",
        "\n",
        "The CIFAR-10 images are classified using the cosine similarity (times 100) as the logits to the softmax operation.\n",
        "\n",
        "OpenAI Clip 's performance on CIFAR-10 as reported by [OpenClip library](https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv#L84) is `89.83%`. However, when we measure the same ourselves, we get `89.59%` which is close. We also experiment how variation in i) class name and ii) text prefixing improves/detoriates performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqu4GlfPfr-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d856e16b-d5d7-4814-c021-bbdc64cf4d44"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 32\n",
        "cifar10 = datasets.CIFAR10(root='~/.cache', train=False, download=True, transform=preprocess)\n",
        "dataloader = torch.utils.data.DataLoader(cifar10, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def get_model_accuracy(model, dataloader, text_tokens, show=True):\n",
        "    top1_accuracies = []\n",
        "    top5_accuracies = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader, desc='Processing batch'):\n",
        "            images = images.to(device)\n",
        "            image_features = model.encode_image(images).float()\n",
        "            text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "            sorted_preds = similarity.T.argsort()\n",
        "\n",
        "            top1_accuracies.extend((sorted_preds[:, -1] == targets).to(torch.int).tolist())\n",
        "            top5_accuracies.extend(np.any((sorted_preds[:, -5:] == targets[:, None]).to(torch.int).cpu().numpy(), axis=1).tolist())\n",
        "\n",
        "    if show:\n",
        "        print(f\"\\nAcc@1: {np.mean(top1_accuracies) * 100:.2f}%\")\n",
        "        print(f\"Acc@5: {np.mean(top5_accuracies) * 100:.2f}%\")\n",
        "\n",
        "    return np.mean(top1_accuracies), np.mean(top5_accuracies)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 46.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: A higher accuracy was achieved by varying the text description as shown in subsequent code\n",
        "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar10.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "top1, top5 = get_model_accuracy(model, dataloader, text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI4q_tsk5nc-",
        "outputId": "a356347d-99ca-40ff-d155-a0c72967ef53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:28<00:00, 11.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 88.96%\n",
            "Acc@5: 99.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating impact of class name change on performance\n",
        "There exist many similar or/and synonym terms for each of the CIFAR-10 classes. We test performance by passing these class name variations. We simplify the experiment by not prefixing any text to the classes. Variation in text prefixing and impact on performance would be seen in subsequent code.\n",
        "1. Airplane: aircraft, plane, jet, airliner, aeroplane\n",
        "2. Automobile: car, vehicle, auto, sedan, motorcar\n",
        "3. Bird: avian, fowl, songbird, feathered friend, winged creature\n",
        "4. Cat: feline, kitty, kitten, tomcat, house cat\n",
        "5. Deer: stag, doe, buck, fawn, antelope\n",
        "6. Dog: canine, puppy, pooch, hound, mutt\n",
        "7. Frog: amphibian, toad, tree frog, bullfrog, croaker\n",
        "8. Horse: equine, steed, pony, stallion, mare\n",
        "9. Ship: vessel, boat, yacht, schooner, liner\n",
        "10. Truck: lorry, pickup, hauler, freight truck, delivery truck\n",
        "\n",
        "We can see that there are benefits to using `variant 1` instead of the original class name as it provides a `0.39%` improvement."
      ],
      "metadata": {
        "id": "ZoHChOzU7rW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_variants = [\n",
        "    [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"],                                 # original cifar 10\n",
        "    [\"aircraft\", \"car\", \"avian\", \"feline\", \"stag\", \"canine\", \"amphibian\", \"equine\", \"vessel\", \"lorry\"],                         # variant 1\n",
        "    [\"plane\", \"vehicle\", \"fowl\", \"kitty\", \"doe\", \"puppy\", \"toad\", \"steed\", \"boat\", \"pickup\"],                                   # variant 2\n",
        "    [\"jet\", \"auto\", \"songbird\", \"kitten\", \"buck\", \"pooch\", \"tree frog\", \"pony\", \"yacht\", \"hauler\"],                             # variant 3\n",
        "    [\"airliner\", \"sedan\", \"feathered friend\", \"tomcat\", \"fawn\", \"hound\", \"bullfrog\", \"stallion\", \"schooner\", \"freight truck\"],  # variant 4\n",
        "    [\"aeroplane\", \"motorcar\", \"winged creature\", \"house cat\", \"antelope\", \"mutt\", \"croaker\", \"mare\", \"liner\", \"delivery truck\"] # variant 5\n",
        "]\n",
        "\n",
        "for class_variant in class_variants:\n",
        "    text_tokens = clip.tokenize(class_variant).to(device)\n",
        "    top1, top5 = get_model_accuracy(model, dataloader, text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay_BA5oH8e2d",
        "outputId": "59096b15-a34a-4f85-ecf7-5f1de35e0db9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:27<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 87.38%\n",
            "Acc@5: 99.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:27<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 87.77%\n",
            "Acc@5: 99.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:26<00:00, 11.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 79.43%\n",
            "Acc@5: 98.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:26<00:00, 11.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 80.64%\n",
            "Acc@5: 98.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:26<00:00, 11.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 74.19%\n",
            "Acc@5: 97.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:26<00:00, 11.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 71.59%\n",
            "Acc@5: 96.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating impact of text description change on performance\n",
        "\n",
        "We obaserve that `variant 3` achieves the best performance at `89.59%`."
      ],
      "metadata": {
        "id": "ASvezD-MPWq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variations involve different phrasing to text description preposition\n",
        "text_descriptions_v0 = [f\"This is a photo of a {label}\" for label in cifar10.classes]           # Tested earlier\n",
        "text_descriptions_v1 = [f\"A picture of a {label}\" for label in cifar10.classes]\n",
        "text_descriptions_v2 = [f\"This is a clear photo of a {label}\" for label in cifar10.classes]\n",
        "text_descriptions_v3 = [f\"An image showing a {label}\" for label in cifar10.classes]             # Highest accuracy\n",
        "text_descriptions_v4 = [f\"A snapshot of a {label}\" for label in cifar10.classes]\n",
        "text_descriptions_v5 = [f\"This is a photo of a {label} in its natural habitat\" for label in cifar10.classes]\n",
        "\n",
        "# Tokenize the text descriptions\n",
        "text_tokens_v0 = clip.tokenize(text_descriptions_v0).to(device)\n",
        "text_tokens_v1 = clip.tokenize(text_descriptions_v1).to(device)\n",
        "text_tokens_v2 = clip.tokenize(text_descriptions_v2).to(device)\n",
        "text_tokens_v3 = clip.tokenize(text_descriptions_v3).to(device)\n",
        "text_tokens_v4 = clip.tokenize(text_descriptions_v4).to(device)\n",
        "text_tokens_v5 = clip.tokenize(text_descriptions_v5).to(device)\n",
        "\n",
        "# Now you can test these variations using your existing code\n",
        "top1_v0, top5_v0 = get_model_accuracy(model, dataloader, text_tokens_v0)\n",
        "top1_v1, top5_v1 = get_model_accuracy(model, dataloader, text_tokens_v1)\n",
        "top1_v2, top5_v2 = get_model_accuracy(model, dataloader, text_tokens_v2)\n",
        "top1_v3, top5_v3 = get_model_accuracy(model, dataloader, text_tokens_v3)\n",
        "top1_v4, top5_v4 = get_model_accuracy(model, dataloader, text_tokens_v4)\n",
        "top1_v5, top5_v5 = get_model_accuracy(model, dataloader, text_tokens_v5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCH5BHSZ6Fn2",
        "outputId": "3a1e2c0f-fb34-4fef-ee41-a3a30c05e24f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:28<00:00, 11.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 88.96%\n",
            "Acc@5: 99.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:29<00:00, 10.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 88.61%\n",
            "Acc@5: 99.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:27<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 87.88%\n",
            "Acc@5: 99.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:26<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 89.59%\n",
            "Acc@5: 99.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:31<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 88.06%\n",
            "Acc@5: 99.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:33<00:00,  9.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 87.39%\n",
            "Acc@5: 98.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We also attempted measuring the performance when using class name variant 1, and text description\n",
        "# variant 3 together as both of them individually achieved highest performance. However, 89.51% remained the best\n",
        "text_descriptions = [f\"An image showing a {label}\" for label in class_variants[1]]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "top1, top5 = get_model_accuracy(model, dataloader, text_tokens)"
      ],
      "metadata": {
        "id": "q3-tGFvqP0JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2828aaf-4682-4dcb-844e-8b5adc48c796"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [00:26<00:00, 11.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 89.31%\n",
            "Acc@5: 99.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiments with augmentation\n",
        "We further attempted improving performance by passing two versions of the same image to the model. The first is the original image and second is an augmented version having horizontal flipping and random rotation transformations applied. We then find similarity with each image and take average across the two. The expectation of achieving a higher accuracy wasn't met with a minor reduction in performance from 89.59% to 89.42%."
      ],
      "metadata": {
        "id": "GNIi3oIC_qQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "new_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=1),\n",
        "    transforms.RandomRotation(15, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    preprocess\n",
        "])\n",
        "\n",
        "batch_size = 32\n",
        "new_dataset = datasets.CIFAR10(root='~/.cache', train=False, download=True, transform=new_transform)\n",
        "new_dataloader = torch.utils.data.DataLoader(new_dataset, batch_size=batch_size, shuffle=False)\n",
        "text_descriptions = [f\"An image showing a {label}\" for label in cifar10.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
        "\n",
        "def get_model_accuracy_2(model, dataloader1, dataloader2, text_tokens, show=True):\n",
        "    top1_accuracies = []\n",
        "    top5_accuracies = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (images1, targets), (images2, _) in tqdm(zip(dataloader1, dataloader2), desc='Processing batch', total=len(dataloader1)):\n",
        "            images1 = images1.to(device)\n",
        "            images2 = images2.to(device)\n",
        "            image_features1 = model.encode_image(images1).float()\n",
        "            image_features2 = model.encode_image(images2).float()\n",
        "            text_features = model.encode_text(text_tokens).float()\n",
        "\n",
        "            image_features1 /= image_features1.norm(dim=-1, keepdim=True)\n",
        "            image_features2 /= image_features2.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity1 = text_features @ image_features1.T\n",
        "            similarity2 = text_features @ image_features2.T\n",
        "            similarity = (similarity1 + similarity2) / 2\n",
        "            sorted_preds = similarity.T.argsort().cpu().numpy()\n",
        "\n",
        "            top1_accuracies.extend((sorted_preds[:, -1] == targets).to(torch.int).tolist())\n",
        "            top5_accuracies.extend(np.any((sorted_preds[:, -5:] == targets[:, None]).to(torch.int).cpu().numpy(), axis=1).tolist())\n",
        "\n",
        "    if show:\n",
        "        print(f\"\\nAcc@1: {np.mean(top1_accuracies) * 100:.2f}%\")\n",
        "        print(f\"Acc@5: {np.mean(top5_accuracies) * 100:.2f}%\")\n",
        "\n",
        "    return np.mean(top1_accuracies), np.mean(top5_accuracies)\n",
        "\n",
        "top1, top5 = get_model_accuracy_2(model, dataloader, new_dataloader, text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3bt1g7M5b8E",
        "outputId": "d751f42f-4af4-4f2b-f4e0-0896681a603d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batch: 100%|██████████| 313/313 [01:01<00:00,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acc@1: 89.44%\n",
            "Acc@5: 99.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear probing results on CIFAR-10\n",
        "\n",
        "The results of `95.02%` are close to `95.1%` in the paper and uses the same hyperparameter values as described in the paper. Some hyper parameters such as L2 lambda were estimated by parametric sweep by the researchers over a log range with some optimisation steps to improve the binary search which wasn't repeated by us as suitable performance was achieved without the same."
      ],
      "metadata": {
        "id": "558V6lcoVF-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "batch_size = 32\n",
        "train_dataset = datasets.CIFAR10(root='~/.cache', train=True, download=True, transform=preprocess)\n",
        "test_dataset = datasets.CIFAR10(root='~/.cache', train=False, download=True, transform=preprocess)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Freeze the parameters of the pretrained model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Utility method for extracting features using the pretrained model\n",
        "def extract_features(loader, desc='Train'):\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(loader, desc):\n",
        "            image_features = model.encode_image(images.to(device)).float()\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            features.append(image_features.cpu()) # NOTE: Only image features considered in linear probe\n",
        "            labels.append(targets)\n",
        "    return torch.cat(features), torch.cat(labels)\n",
        "\n",
        "train_features, train_labels = extract_features(train_loader, desc='Train')\n",
        "test_features, test_labels = extract_features(test_loader, desc='Test')\n",
        "\n",
        "# Flatten the features to use with LogisticRegression\n",
        "train_features = train_features.view(train_features.size(0), -1).numpy()\n",
        "test_features = test_features.view(test_features.size(0), -1).numpy()\n",
        "train_labels = train_labels.numpy()\n",
        "test_labels = test_labels.numpy()\n",
        "\n",
        "# This range was defined for L2 regularization strength (λ) in the CLIP paper\n",
        "lambda_range = np.logspace(-6, 6, 96)\n",
        "\n",
        "# Perform logistic regression with cross-validation to find the best λ\n",
        "clf = LogisticRegressionCV(\n",
        "    Cs=lambda_range,\n",
        "    cv=5,\n",
        "    max_iter=1000,\n",
        "    solver='lbfgs',\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "clf.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = clf.predict(test_features)\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEtzURqrXkEF",
        "outputId": "10cb5f81-9ea2-4b4c-feac-b8fcb9521b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 3125/3125 [02:19<00:00, 22.39it/s]\n",
            "Test: 100%|██████████| 625/625 [00:28<00:00, 22.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.02%\n"
          ]
        }
      ]
    }
  ]
}